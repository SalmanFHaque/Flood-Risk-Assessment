{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Data Import"
      ],
      "metadata": {
        "id": "xEQtWCh2zTX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "kZ2ZaYpAzTAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insert Necessary Library"
      ],
      "metadata": {
        "id": "A3d1x2cq0B9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "j-re0PTA0BFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Signature Database"
      ],
      "metadata": {
        "id": "n2GNs6U50PsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sdf = pd.read_csv(\"800_Final.csv\")\n",
        "sdf"
      ],
      "metadata": {
        "id": "NXrXOyte0PHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g47FznThzN6z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Random Dataset"
      ],
      "metadata": {
        "id": "YfQJR6o50cPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create a random dataset with 12 factors\n",
        "np.random.seed(5)\n",
        "data = {\n",
        "    'Slope': np.random.rand(100),\n",
        "    'Aspect': np.random.rand(100),\n",
        "    'Curvature': np.random.rand(100),\n",
        "    'TWI': np.random.rand(100),\n",
        "    'TRI': np.random.rand(100),\n",
        "    'Drainage_Density': np.random.rand(100),\n",
        "    'River': np.random.rand(100),\n",
        "    'Annual_Rainfall': np.random.rand(100),\n",
        "    'DEM': np.random.rand(100),\n",
        "    'NDVI': np.random.rand(100),\n",
        "    'Soil_Texture': np.random.rand(100),\n",
        "    'Geology': np.random.rand(100)\n",
        "}\n",
        "\n",
        "# Create a DataFrame from the dataset\n",
        "sdf = pd.DataFrame(data)\n"
      ],
      "metadata": {
        "id": "7mFu51S10kl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding Out Correlation Matrix for the Selected Features"
      ],
      "metadata": {
        "id": "CjVjdnWX0qet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix = sdf.corr()"
      ],
      "metadata": {
        "id": "uo7CO2gv0m1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation Matrix Plot"
      ],
      "metadata": {
        "id": "g0nz3_tV02Cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the figure size (optional)\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Create a heatmap\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
        "\n",
        "# Add a title\n",
        "plt.title('Correlation Matrix of 12 Factors')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sZPoVJnX0mx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalized Correlation Matrix"
      ],
      "metadata": {
        "id": "DQqUKdlg059a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the correlation matrix\n",
        "normalized_corr_matrix = (correlation_matrix - correlation_matrix.min().min()) / (correlation_matrix.max().max() - correlation_matrix.min().min())\n"
      ],
      "metadata": {
        "id": "P170lqBJ0mu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the figure size (optional)\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Create a heatmap\n",
        "sns.heatmap(normalized_corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "\n",
        "# Add a title\n",
        "plt.title('Normalized Correlation Heatmap of 12 Factors')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TvXzlVnT1Dpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Data Fill Up"
      ],
      "metadata": {
        "id": "PhTqlMUd1DLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dff = sdf.fillna(method='ffill')\n",
        "dff"
      ],
      "metadata": {
        "id": "KGD7rq3F1V6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export the Data as CSV"
      ],
      "metadata": {
        "id": "sQK6Yced1fhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to save the CSV file in your Google Drive\n",
        "output_path = '800_Final.csv'\n",
        "\n",
        "# Export the DataFrame to a CSV file in your Google Drive\n",
        "dff.to_csv(output_path, index=False)"
      ],
      "metadata": {
        "id": "NP8lYecc1W0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Dependent Value and the Independent Value"
      ],
      "metadata": {
        "id": "kZw9tVqU1i4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = sdf[['DEM','NDVI','Slope','TWI', 'TRI','Aspect', 'Soil_Texture_Final', 'Curvature' ,'Drainage_Density', 'River_ED', 'Annual_Rainfall', 'Geology_Final']]\n",
        "Y = sdf['FloodYN']"
      ],
      "metadata": {
        "id": "Ps4LdlTx2A3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the Number of Dataset for Train, Test and Split"
      ],
      "metadata": {
        "id": "JgTKlKRw2LRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, test_X, train_Y, test_Y = train_test_split(\n",
        "    X, Y, train_size=0.7, random_state=5)"
      ],
      "metadata": {
        "id": "1v_lunMv2IzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_Y"
      ],
      "metadata": {
        "id": "w68MXh8r2SVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_Y"
      ],
      "metadata": {
        "id": "ipOePktE2SSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X"
      ],
      "metadata": {
        "id": "MhrDHcpr2SPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_X"
      ],
      "metadata": {
        "id": "7Al4naDR2SMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Our Database is Ready, Now we need to Import the full Database to Predict the Data for whole database for different ML Algorithm**"
      ],
      "metadata": {
        "id": "tqwxr5BC2eAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full Database Import"
      ],
      "metadata": {
        "id": "CmeCVm8K2z1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "kSjA6iG_2aEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"R.csv\")\n",
        "df"
      ],
      "metadata": {
        "id": "wWLZ8azc24X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_X = df[['DEM','NDVI','Slope','TWI', 'TRI','Aspect', 'Soil_Texture_Final', 'Curvature' ,'Drainage_Density', 'River_ED', 'Annual_Rainfall', 'Geology_Final']]\n",
        "full_Y = df['FloodYN']"
      ],
      "metadata": {
        "id": "btbmf5EY24Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inserting the Previously Assisgned Train and Test Dataset"
      ],
      "metadata": {
        "id": "sQ0884rs3NlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, test_X, train_Y, test_Y = train_test_split(\n",
        "    X, Y, train_size=0.7, random_state=5)"
      ],
      "metadata": {
        "id": "uAgWb9Qu29Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Equal Seperation"
      ],
      "metadata": {
        "id": "ttfOvT-t4JD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# Assuming you have your data in X and Y dataframes\n",
        "\n",
        "# Calculate the number of samples with 0 and 1 in the dependent variable\n",
        "num_samples_0 = len(Y[Y == 0])\n",
        "num_samples_1 = len(Y[Y == 1])\n",
        "\n",
        "# Calculate the minimum number of samples to use for both classes\n",
        "min_samples = min(num_samples_0, num_samples_1)\n",
        "\n",
        "# Choose the number of samples you want for the test set (e.g., 5000)\n",
        "num_samples_test = 240\n",
        "\n",
        "# Ensure you don't exceed the minimum number of samples available for either class\n",
        "num_samples_test = min(num_samples_test, min_samples)\n",
        "\n",
        "# Create a mask for selecting samples with 0 and 1 in the dependent variable\n",
        "mask_0 = Y == 0\n",
        "mask_1 = Y == 1\n",
        "\n",
        "# Sample an equal number of samples from both classes for the test set\n",
        "test_indices_0 = np.random.choice(np.where(mask_0)[0], num_samples_test // 2, replace=False)\n",
        "test_indices_1 = np.random.choice(np.where(mask_1)[0], num_samples_test // 2, replace=False)\n",
        "\n",
        "# Concatenate the indices for the test set\n",
        "test_indices = np.concatenate([test_indices_0, test_indices_1])\n",
        "\n",
        "# Create train and test dataframes\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=num_samples_test, stratify=Y, random_state=5)\n",
        "\n",
        "# Filter the test data based on the selected indices\n",
        "test_X = X.iloc[test_indices]\n",
        "test_Y = Y.iloc[test_indices]"
      ],
      "metadata": {
        "id": "-N7-sy0z24Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating the ML Algorithm and its Accuracy**"
      ],
      "metadata": {
        "id": "UTYkphOn4XrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Model"
      ],
      "metadata": {
        "id": "JfT0sA8i4dKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Generate some sample data\n",
        "np.random.seed(5)\n",
        "\n",
        "# Create a Random Forest Regressor model\n",
        "n_estimators = 100  # Number of trees in the forest\n",
        "max_depth = None    # Maximum depth of the trees, None means unlimited depth\n",
        "random_forest = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=5)\n",
        "\n",
        "# Train the model\n",
        "random_forest.fit(train_X, train_Y)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = random_forest.predict(test_X)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(test_Y, y_pred)\n",
        "r2 = r2_score(test_Y, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "\n",
        "sdf[\"predictedRF\"] = random_forest.predict(X)\n",
        "sdf\n"
      ],
      "metadata": {
        "id": "XA4ORYWQ4fdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Importance for the Random Forest Model"
      ],
      "metadata": {
        "id": "DnADH28V4gXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Get feature importances from the trained model\n",
        "feature_importances = random_forest.feature_importances_\n",
        "\n",
        "# Get the corresponding feature names or column names if you have them\n",
        "feature_names = X.columns  # Replace with your feature names if available\n",
        "\n",
        "# Sort the feature importances in descending order\n",
        "sorted_idx = np.argsort(feature_importances)[::-1]\n",
        "\n",
        "# Print feature importances and feature names\n",
        "for i in sorted_idx:\n",
        "    print(f\"{feature_names[i]}: {feature_importances[i]}\")\n",
        "\n",
        "# Create a bar plot to visualize feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Feature Importances\")\n",
        "plt.bar(range(X.shape[1]), feature_importances[sorted_idx], align=\"center\")\n",
        "plt.xticks(range(X.shape[1]), np.array(feature_names)[sorted_idx], rotation=90)\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hL4ngvAd4pW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_X = df[['DEM','NDVI','Slope','TWI', 'TRI','Aspect', 'Soil_Texture_Final', 'Curvature' ,'Drainage_Density', 'River_ED', 'Annual_Rainfall', 'Geology_Final']]\n",
        "full_Y = df['FloodYN']"
      ],
      "metadata": {
        "id": "WPpgVhol41Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_X"
      ],
      "metadata": {
        "id": "QthpL9tX409J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicted Column Adding with the Full Dataset"
      ],
      "metadata": {
        "id": "f78FBWcC46DB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_X = df[['DEM','NDVI','Slope','TWI', 'TRI','Aspect', 'Soil_Texture_Final', 'Curvature' ,'Drainage_Density', 'River_ED', 'Annual_Rainfall', 'Geology_Final']]\n",
        "df[\"PredictedRF\"] = random_forest.predict(full_X)\n",
        "df"
      ],
      "metadata": {
        "id": "Y8USPp0m43Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision Tree Model**"
      ],
      "metadata": {
        "id": "PksvtH8V5E3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = sdf[['DEM','NDVI','Slope','TWI', 'TRI','Aspect', 'Soil_Texture_Final', 'Curvature' ,'Drainage_Density', 'River_ED', 'Annual_Rainfall', 'Geology_Final']]\n",
        "Y = sdf['FloodYN']"
      ],
      "metadata": {
        "id": "vOrsLr8K43Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, test_X, train_Y, test_Y = train_test_split(\n",
        "    X, Y, train_size=0.7, random_state=5)"
      ],
      "metadata": {
        "id": "ingEhhzx5ss5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# Assuming you have your data in X and Y dataframes\n",
        "\n",
        "# Calculate the number of samples with 0 and 1 in the dependent variable\n",
        "num_samples_0 = len(Y[Y == 0])\n",
        "num_samples_1 = len(Y[Y == 1])\n",
        "\n",
        "# Calculate the minimum number of samples to use for both classes\n",
        "min_samples = min(num_samples_0, num_samples_1)\n",
        "\n",
        "# Choose the number of samples you want for the test set (e.g., 5000)\n",
        "num_samples_test = 240\n",
        "\n",
        "# Ensure you don't exceed the minimum number of samples available for either class\n",
        "num_samples_test = min(num_samples_test, min_samples)\n",
        "\n",
        "# Create a mask for selecting samples with 0 and 1 in the dependent variable\n",
        "mask_0 = Y == 0\n",
        "mask_1 = Y == 1\n",
        "\n",
        "# Sample an equal number of samples from both classes for the test set\n",
        "test_indices_0 = np.random.choice(np.where(mask_0)[0], num_samples_test // 2, replace=False)\n",
        "test_indices_1 = np.random.choice(np.where(mask_1)[0], num_samples_test // 2, replace=False)\n",
        "\n",
        "# Concatenate the indices for the test set\n",
        "test_indices = np.concatenate([test_indices_0, test_indices_1])\n",
        "\n",
        "# Create train and test dataframes\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=num_samples_test, stratify=Y, random_state=5)\n",
        "\n",
        "# Filter the test data based on the selected indices\n",
        "test_X = X.iloc[test_indices]\n",
        "test_Y = Y.iloc[test_indices]"
      ],
      "metadata": {
        "id": "NkF93Ndd5spx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_X = df[['DEM','NDVI','Slope','TWI', 'TRI','Aspect', 'Soil_Texture_Final', 'Curvature' ,'Drainage_Density', 'River_ED', 'Annual_Rainfall', 'Geology_Final']]\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Generate some sample data\n",
        "np.random.seed(5)\n",
        "\n",
        "# Create a Decision Tree Regressor model\n",
        "max_depth = 5  # Maximum depth of the tree\n",
        "decision_tree = DecisionTreeRegressor(max_depth=max_depth)\n",
        "\n",
        "# Train the model\n",
        "decision_tree.fit(train_X, train_Y)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = decision_tree.predict(test_X)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(test_Y, y_pred)\n",
        "r2 = r2_score(test_Y, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "\n",
        "sdf[\"predictedDT\"] = decision_tree.predict(X)\n",
        "sdf\n"
      ],
      "metadata": {
        "id": "Agx1LsSr5snJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Factor Importance for the Decision Tree Model"
      ],
      "metadata": {
        "id": "LGZK2yP-58-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Get feature importances from the trained model\n",
        "feature_importances = decision_tree.feature_importances_\n",
        "\n",
        "# Get the corresponding feature names or column names if you have them\n",
        "feature_names = X.columns  # Replace with your feature names if available\n",
        "\n",
        "# Sort the feature importances in descending order\n",
        "sorted_idx = np.argsort(feature_importances)[::-1]\n",
        "\n",
        "# Print feature importances and feature names\n",
        "for i in sorted_idx:\n",
        "    print(f\"{feature_names[i]}: {feature_importances[i]}\")\n",
        "\n",
        "# Create a bar plot to visualize feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Feature Importances\")\n",
        "plt.bar(range(X.shape[1]), feature_importances[sorted_idx], align=\"center\")\n",
        "plt.xticks(range(X.shape[1]), np.array(feature_names)[sorted_idx], rotation=90)\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UIUTDZBQ54tQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_X"
      ],
      "metadata": {
        "id": "AtQqqFEy5skE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"PredictedDT\"] = decision_tree.predict(full_X)\n",
        "df"
      ],
      "metadata": {
        "id": "H0FTyEdM6CeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KNN Model**"
      ],
      "metadata": {
        "id": "pX0WESNG4tgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = sdf[['DEM','NDVI','Slope','TWI', 'TRI','Aspect', 'Soil_Texture_Final', 'Curvature' ,'Drainage_Density', 'River_ED', 'Annual_Rainfall', 'Geology_Final']]\n",
        "Y = sdf['FloodYN']"
      ],
      "metadata": {
        "id": "P9hWFZ9J4bF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, test_X, train_Y, test_Y = train_test_split(\n",
        "    X, Y, train_size=0.7, random_state=5)"
      ],
      "metadata": {
        "id": "kyviFdpG6USY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Assuming you have your data in X and Y dataframes\n",
        "\n",
        "# Calculate the number of samples with 0 and 1 in the dependent variable\n",
        "num_samples_0 = len(Y[Y == 0])\n",
        "num_samples_1 = len(Y[Y == 1])\n",
        "\n",
        "# Calculate the minimum number of samples to use for both classes\n",
        "min_samples = min(num_samples_0, num_samples_1)\n",
        "\n",
        "# Choose the number of samples you want for the test set (e.g., 5000)\n",
        "num_samples_test = 240\n",
        "\n",
        "# Ensure you don't exceed the minimum number of samples available for either class\n",
        "num_samples_test = min(num_samples_test, min_samples)\n",
        "\n",
        "# Create a mask for selecting samples with 0 and 1 in the dependent variable\n",
        "mask_0 = Y == 0\n",
        "mask_1 = Y == 1\n",
        "\n",
        "# Sample an equal number of samples from both classes for the test set\n",
        "test_indices_0 = np.random.choice(np.where(mask_0)[0], num_samples_test // 2, replace=False)\n",
        "test_indices_1 = np.random.choice(np.where(mask_1)[0], num_samples_test // 2, replace=False)\n",
        "\n",
        "# Concatenate the indices for the test set\n",
        "test_indices = np.concatenate([test_indices_0, test_indices_1])\n",
        "\n",
        "# Create train and test dataframes\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=num_samples_test, stratify=Y, random_state=5)\n",
        "\n",
        "# Filter the test data based on the selected indices\n",
        "test_X = X.iloc[test_indices]\n",
        "test_Y = Y.iloc[test_indices]"
      ],
      "metadata": {
        "id": "pitmmu_F6VmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "#full_X = df[['DEM','NDVI','Slope','TWI', 'TRI','Aspect', 'Soil_Texture_Final', 'Curvature' ,'Drainage_Density', 'River_ED', 'Annual_Rainfall', 'Geology_Final']]\n",
        "# Generate some sample data\n",
        "np.random.seed(5)\n",
        "\n",
        "# Create a K Nearest Neighbors Regressor model\n",
        "n_neighbors = 5  # Number of neighbors to consider\n",
        "knn_regressor = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
        "\n",
        "# Train the model\n",
        "knn_regressor.fit(train_X, train_Y)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = knn_regressor.predict(test_X)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(test_Y, y_pred)\n",
        "r2 = r2_score(test_Y, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "\n",
        "sdf[\"predictedDT\"] = knn_regressor.predict(X)\n",
        "sdf\n"
      ],
      "metadata": {
        "id": "6xSQAoLH6Xfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Get feature importances from the trained model\n",
        "feature_importances = knn_regressor.feature_importances_\n",
        "\n",
        "# Get the corresponding feature names or column names if you have them\n",
        "feature_names = X.columns  # Replace with your feature names if available\n",
        "\n",
        "# Sort the feature importances in descending order\n",
        "sorted_idx = np.argsort(feature_importances)[::-1]\n",
        "\n",
        "# Print feature importances and feature names\n",
        "for i in sorted_idx:\n",
        "    print(f\"{feature_names[i]}: {feature_importances[i]}\")\n",
        "\n",
        "# Create a bar plot to visualize feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Feature Importances\")\n",
        "plt.bar(range(X.shape[1]), feature_importances[sorted_idx], align=\"center\")\n",
        "plt.xticks(range(X.shape[1]), np.array(feature_names)[sorted_idx], rotation=90)\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KA_7_YXx6ZK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"PredictedKNN\"] = knn_regressor.predict(full_X)\n",
        "df"
      ],
      "metadata": {
        "id": "gu812aRy6djU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XGBoost Model**"
      ],
      "metadata": {
        "id": "ohKrSxkP6hYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = sdf[['DEM','NDVI','Slope','TWI', 'TRI','Aspect', 'Soil_Texture_Final', 'Curvature' ,'Drainage_Density', 'River_ED', 'Annual_Rainfall', 'Geology_Final']]\n",
        "Y = sdf['FloodYN']"
      ],
      "metadata": {
        "id": "ud1po52M6koQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, test_X, train_Y, test_Y = train_test_split(\n",
        "    X, Y, train_size=0.7, random_state=5)"
      ],
      "metadata": {
        "id": "TY11Hb_l6mrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Assuming you have your data in X and Y dataframes\n",
        "\n",
        "# Calculate the number of samples with 0 and 1 in the dependent variable\n",
        "num_samples_0 = len(Y[Y == 0])\n",
        "num_samples_1 = len(Y[Y == 1])\n",
        "\n",
        "# Calculate the minimum number of samples to use for both classes\n",
        "min_samples = min(num_samples_0, num_samples_1)\n",
        "\n",
        "# Choose the number of samples you want for the test set (e.g., 5000)\n",
        "num_samples_test = 240\n",
        "\n",
        "# Ensure you don't exceed the minimum number of samples available for either class\n",
        "num_samples_test = min(num_samples_test, min_samples)\n",
        "\n",
        "# Create a mask for selecting samples with 0 and 1 in the dependent variable\n",
        "mask_0 = Y == 0\n",
        "mask_1 = Y == 1\n",
        "\n",
        "# Sample an equal number of samples from both classes for the test set\n",
        "test_indices_0 = np.random.choice(np.where(mask_0)[0], num_samples_test // 2, replace=False)\n",
        "test_indices_1 = np.random.choice(np.where(mask_1)[0], num_samples_test // 2, replace=False)\n",
        "\n",
        "# Concatenate the indices for the test set\n",
        "test_indices = np.concatenate([test_indices_0, test_indices_1])\n",
        "\n",
        "# Create train and test dataframes\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=num_samples_test, stratify=Y, random_state=5)\n",
        "\n",
        "# Filter the test data based on the selected indices\n",
        "test_X = X.iloc[test_indices]\n",
        "test_Y = Y.iloc[test_indices]"
      ],
      "metadata": {
        "id": "c1SQcS3o6moA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import MinMaxScaler  # Import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "#full_X = df[['DEM','NDVI','Slope','TWI', 'TRI','Aspect', 'Soil_Texture_Final', 'Curvature' ,'Drainage_Density', 'River_ED', 'Annual_Rainfall', 'Geology_Final']]\n",
        "# Generate some sample data\n",
        "np.random.seed(5)\n",
        "\n",
        "# Create an XGBoost Regressor model\n",
        "xgb_regressor = xgb.XGBRegressor(objective='reg:squarederror', random_state=5)\n",
        "\n",
        "# Train the model\n",
        "xgb_regressor.fit(train_X, train_Y)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = xgb_regressor.predict(test_X)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(test_Y, y_pred)\n",
        "r2 = r2_score(test_Y, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "\n",
        "sdf[\"predictedXG\"] = xgb_regressor.predict(X)\n",
        "sdf\n"
      ],
      "metadata": {
        "id": "piySygJN6miI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"predictionsXGP\"] = xgb_regressor.predict(full_X)\n",
        "df"
      ],
      "metadata": {
        "id": "IhvDG0cw6sZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"XGBoost\"]=((df.predictionsXGP-df.predictionsXGP.min())/(df.predictionsXGP.max()-df.predictionsXGP.min()))\n",
        "df"
      ],
      "metadata": {
        "id": "jsxo0P8e6xqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LR Model"
      ],
      "metadata": {
        "id": "EgG9qkZI7GHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = sdf[['DEM','NDVI','Slope','TWI', 'TRI','Aspect', 'Soil_Texture_Final', 'Curvature' ,'Drainage_Density', 'River_ED', 'Annual_Rainfall', 'Geology_Final']]\n",
        "Y = sdf['FloodYN']"
      ],
      "metadata": {
        "id": "A9MEgI786xjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, test_X, train_Y, test_Y = train_test_split(\n",
        "    X, Y, train_size=0.7, random_state=5)"
      ],
      "metadata": {
        "id": "FAtoWPra6xmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# Assuming you have your data in X and Y dataframes\n",
        "\n",
        "# Calculate the number of samples with 0 and 1 in the dependent variable\n",
        "num_samples_0 = len(Y[Y == 0])\n",
        "num_samples_1 = len(Y[Y == 1])\n",
        "\n",
        "# Calculate the minimum number of samples to use for both classes\n",
        "min_samples = min(num_samples_0, num_samples_1)\n",
        "\n",
        "# Choose the number of samples you want for the test set (e.g., 5000)\n",
        "num_samples_test = 240\n",
        "\n",
        "# Ensure you don't exceed the minimum number of samples available for either class\n",
        "num_samples_test = min(num_samples_test, min_samples)\n",
        "\n",
        "# Create a mask for selecting samples with 0 and 1 in the dependent variable\n",
        "mask_0 = Y == 0\n",
        "mask_1 = Y == 1\n",
        "\n",
        "# Sample an equal number of samples from both classes for the test set\n",
        "test_indices_0 = np.random.choice(np.where(mask_0)[0], num_samples_test // 2, replace=False)\n",
        "test_indices_1 = np.random.choice(np.where(mask_1)[0], num_samples_test // 2, replace=False)\n",
        "\n",
        "# Concatenate the indices for the test set\n",
        "test_indices = np.concatenate([test_indices_0, test_indices_1])\n",
        "\n",
        "# Create train and test dataframes\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=num_samples_test, stratify=Y, random_state=5)\n",
        "\n",
        "# Filter the test data based on the selected indices\n",
        "test_X = X.iloc[test_indices]\n",
        "test_Y = Y.iloc[test_indices]"
      ],
      "metadata": {
        "id": "m8fYIA2k7MlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MinMaxScaler  # Import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Generate some sample data\n",
        "np.random.seed(5)\n",
        "\n",
        "# Convert y to binary labels based on a threshold\n",
        "threshold = np.mean(Y)\n",
        "binary_labels = (Y >= threshold).astype(int)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, binary_labels, train_size=560, random_state=5)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "logreg.fit(train_X, train_Y)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_prob = logreg.predict_proba(test_X)[:, 1]  # Probability of class 1\n",
        "y_pred = y_pred_prob * (Y.max() - Y.min()) + Y.min()  # Convert probabilities to regression predictions\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(test_Y, y_pred)\n",
        "r2 = r2_score(test_Y, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "\n",
        "sdf[\"predictedLR\"] = logreg.predict_proba(X)[:, 1]\n",
        "sdf\n"
      ],
      "metadata": {
        "id": "LQlMUndw7Mgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can explore more ML Algorithm **(SVM, ANN, Gradient Boosting and etc.)** to find out their Accuracy"
      ],
      "metadata": {
        "id": "WOWPlXlS7fSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export the CSV in Drive"
      ],
      "metadata": {
        "id": "qAr3qdPx6vDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to save the CSV file in your Google Drive\n",
        "output_path = 'Final.csv'\n",
        "\n",
        "# Export the DataFrame to a CSV file in your Google Drive\n",
        "df.to_csv(output_path, index=False)"
      ],
      "metadata": {
        "id": "B7Ysumhx7E7i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}